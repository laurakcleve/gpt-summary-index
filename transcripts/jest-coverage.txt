

Welcome to another Swashbuckling with Code. I'm Jimmy Cleveland, and today we're going to go over Jest and its test coverage functionality, and we're going to use these coverage reports to sort of clue us into what we should be testing next and how much confidence do we have in our codebase. As you'll see throughout this video, I'm a big fan of coverage reports, and in particular using it in a sort of continuous integration fashion through your GitHub Actions or your Bitbucket Pipelines or whatnot. I'm going to be showing a little bit of that toward the end just so you can see the sort of cool functionality of it, but for the rest of it, what we're going to go over is how to use a coverage report, how to configure it to let you know how many files in your project are lacking coverage based on your specifications of what you want to cover, and we'll also configure some thresholds for what we consider to be sufficient for not only, you know, global coverage of the whole project itself, but maybe individual files or folders that need a little bit higher or a little bit lower coverage. I think to understand it better than that, it's easier just to see it in action, so let's start. So I've gone ahead and created this example repo to speed things up, and let me take you for a spin of it so you kind of know what's going on here. First thing you're going to want to see is that we have these scripts, and these scripts are going to say test, test watch, and test CI. So this is just your standard Jest run. This is going to be a Jest with watch just because I don't want to pass the additional flags every time, and the CI one in particular is going to be for our continuous integration, which is pretty standard, and that's going to be delved into a little bit later with GitHub actions. The only other thing you really need to know in this file is that we have Jest installed as a dev dependency. These are the only dependencies we have at all, any packages, and this at types Jest. If you're wondering why that's there, that's typically because if you're not getting auto completion and VS code for all of your Jest statements like expect and stuff like that, if you install the types for it, then you typically will. Now the next step is going to be the source folder, and we have various different things. We have a utils folder with a few things, and then at the root we have just this generate item, and we already have a single test written for it. Now each of these methods I'm going to go into, or I should say files, I'm going to go into and look at all of the different functions in them as we need to. But for just a start, what we are starting with is this silly example I've created of a generate item that I put just a bunch of conditional checks for demo purposes. And so you're going to call it, and it's going to give you back an object of like a base weapon or a base armor with some random generated stats. It's probably not the greatest example in the world, but you'd be surprised at how long it takes to come up with a decent example that is, you know, a good balance between contrived so that it is understandable, you know, but kind of not really realistic. But if it's too realistic, then it takes too long to get into and really understand all of it. So I think that this strikes a pretty good balance, hopefully. So just bear with me. So at this point, I'm going to hop back over to the terminal here, and I'm going to run npm t for just a shorthand for running the test command. And you can see that all our tests pass. Now the scenarios that I want you to imagine here as we're going through this code is that you're either a person who just wrote this, and you're kind of wondering, what should I test next? Or let's say that you are a maintainer or a co developer or someone else on a team that peer reviews this code. And you're trying to kind of figure out what's the stability of it, or maybe you're wanting to either refactor or fix some of the code. And you want to know what parts you can remove. And a really good first step to that is writing tests for those things. And I think that coverage really comes in fantastically here, because it's going to give you a good idea of what all of the code that is in the code base is actually covered by tests. Now I have some notes on what covered actually means, but I think it'd be better for me to just kind of show you what it looks like first, and then we can talk to that aspect. So what we can do is we can take our npm t command or test, and then we can use the dash dash to pass another flag to it. And then we can say dash dash coverage. And there you'll see that we'll get our first very interesting coverage report, which is nicely printed out in the terminal. So that's the usage of the dash dash coverage property. And if you're on yarn, you could actually just do yarn test dash dash coverage, you don't have to pass the additional dash dash just so you know. And of course, you could also make a script that does that for you. Now at a quick glance, we get some pretty good information here, we can see that it says that source utils is completely covered. The helper JS is the one that it's tested in there. And we'll come back to that in a moment. But the statements, branches, functions, and lines are 100% covered. So you know, statements are any individual statement of code lines, it the lines are going to be a little bit different than statements sometimes, but usually they kind of line up functions, obviously, is pretty obvious, I think, and then branches are just conditional logic, you know, such as ternaries, or if statements or switch statements or anything like that. It's even nice because it shows you the covered line numbers, in case you just want to zip right over to those. But you can see in the rest of our stuff, this, this all files is telling us this is like global coverage, this is what it considers our entire code base that's tested. And it's telling us that, you know, only 35% of our branches are covered, which it's giving the red color to kind of alert you to that. And then, you know, we're getting into the warning state for some of this other stuff. If we go in individually to generate item, you can see this is pretty feature paired up with that. And that so that's a pretty good indicator that this is largely affecting that. So let's go look at the generate item test real quick. So if we go into here, we'll see that we have a test right now. And it pulls in generate item. And then from there, what it does is we have a should create a weapon object test, we have an expected weapon. And this is just a pretty normal, like, here's kind of a generic shape. I let's kind of go through the test, actually. So we expect that weapon, which is going to be the result of calling generate item with the config of type weapon and the rarity epic, we want that to match an object. And there's various different ways to do that. Don't worry about that for the purpose of this video. But we're going to say, okay, I want it to match this expected weapon object. And so it should come back with a type weapon and a rarity epic, which is what we passed in. But it also should have at least a damage and a damage type. And that can be any number or any string. And that's because those are going to be randomly generated. So we can't 100% predict that at this point. If you want to see just really quickly kind of what that looks like, let's do a console dot log of weapon, what that comes back as, I'll come over to here, and we'll just run npm t once. And here you can see this is generally what it comes back. So it's going to say the type is weapon, the damage is piercing damage 19 attack speed zero, as that's not written in yet. And then rarity is epic. So we're kind of in the middle of building this thing, you can think of it that way. So at this point, whether you've written the code yourself, and you're coming back to it a later date, or you're doing it now, or like I said, you're a maintainer, you're kind of wondering, all right, what should I test next? Okay, so what you could do is you could obviously go into your generate item. And if you just wrote this code, you'd probably be able to guess that. Well, we generate out a sort of a weapon and an armor. So we probably need to test armor, right? But if you're a maintainer, or you've come back to it from a little bit in the future, it might be a little bit of like mental downloading to kind of get in here and look at all this code and all these functions and be like, Okay, yeah, there's a lot of stuff going on here. And okay, now I start to kind of get you know, what's going on. But you can kind of forego all of that. This is what I really like about coverage is with our coverage report. Let's do this. So if you go to the coverage folder, which has been generated for you, when you run the dash dash coverage flag, you can look into this l curve report. And there's going to be an index HTML. Now I'm just going to like reveal this and Explorer as a way to open it. Move that over here. So you can see that. And then I'm just going to open this file, just that simple. And what's cool is from this point on, I can then reload this anytime I rewrote my code. I wish there was a dark mode for this is blinding me. So we can see that it gives us this nice little diagram sheet. And you can see that source utils is 100% covered. Let's actually just take a quick peek into that. So if you go into source utils, we can see helper j s, everything's covered, everything's green and beautiful. And if we look at it, it tells us, oh, this is interesting, like this ran 21 times, somehow, and then 111, and then two x for this module that exports. Okay, so let's take a look. Why is that? Well, if we go back over to our utils, we have a test for helpers test. And you can see that what it's doing, because it's a randomly generated thing, we are looping over it this 20 ish times in this case, 21. And we're going to go and call it and we're going to get a random int of three, and we expect that to be greater than or equal to zero. So we know that it's going to start at zero, and then it should be less than three. This is a really nice thing that I like about tests just in general is they can kind of be a sort of documentation. If you're getting into a project or reviewing code, because this tells me immediately like, what the qualifications of this function are. In this case, I know that it is exclusive in the upper bound, which means that it needs to be less than three, even though we passed in three. And I know that it probably starts at and is inclusive at zero. So of course, I could figure that out from the code. But it's nice to get a little bit of confirmation. And this test pass after running at 20 or so times. So I've got a reasonable amount of confidence, you could run that as many times as you like. But that's the gist. So back to our report here. Let's now we can see why that lets you know that that test is running out that many times. Let's go back a bit here and look in our trouble area. So if we go to this generate item, we know that we've got some problems here. There's clearly some colors that we don't like typically. And when we're digging into this code, I've got this zoomed in a little bit for you. Normally, it'd be a lot smaller. Let me show you actually, it's like that small. I'll bump this up a tiny bit, or a large bit. And as we're looking through, this is actually the code that's being tested by that test file that we ran. And we only had that one test. But you can see it actually covered quite a bit of the code base. And what's covered is just basically the non highlighted sections, you can think of it that way. So it's run through and ran each of these, these one times. And this is where we first want to start whenever you see this little flag. This is an else path not taken. I know that's really tiny, it doesn't zoom that in for you. And what this is trying to tell us is that we know when you run this if condition, you know, some of these else's are not even run. So you're not really testing the full extent of this generate stat function that's in that file. You're only really testing for an epic rarity here. And it does this nice job of also highlighting that here for you. So statement not covered when you hover over that. And yeah, so that's a that's a pretty good just quick flag to say, all right, well, I know now that our tests don't even cover these examples. So I'm not feeling very confident that this function works to the full extent. Let's keep going down, though. So now we can see that this one generate base armor isn't covered at all, we hover over it, it says function not covered. So that's the functions part of the equation of the stats that come back to us. And it's just going to highlight that return just to show Yeah, you know, even though of course, the function wasn't covered, the statement wasn't covered either. Coming down here, this is a really nice one to get this if path not taken. So this is a sort of short circuit type of command to say like, hey, if you call generate item, and you didn't pass in a type, you know, in this case, they're just going to return null, maybe they return an error, or maybe they turn return, whatever, this is a condition that we really want to test, we don't only want to test what's known as like the happy path of, you know, the path to success of a function, we also want to test the ways in which the user might use it incorrectly. And how do we cover that? Do we fail gracefully? Do we throw an error like we intend? And so this is good to, you know, be notified that like, you know, you're, you're not even covering that either, you don't even run a test. This is kind of getting into the meaning of covered. And I'll finish it after I look at this quick line right here. This is just saying, same thing else path not taken, we can see that we do check base type item is a weapon. And then you know, that's successful, because we're generating on a weapon. And it runs this line of code, but we don't run this else, which is very clearly at this point for armor. So this sort of leads me into what does covered mean? Because it's very important, once you're getting into coverage, you can see the benefits right away, I think, as you start to dig into it. But what does it actually really mean? Well, it just means that when they say covered, it means that that code was run in a test. It doesn't mean that you've covered every possible scenario, that you have all of the functionality sufficiently covered, I guess is a better way to word that. So it doesn't really mean that your tests are even good. It just means that the code was run, it's kind of a bare minimum to say, is this even in a test? Does a test even run this code? And because you would get an error in some situations. And you know, if you've never run that in a test, then the test isn't going to fail on a simple error. So that's kind of like the bare minimum version of a test is does the code run and not throw an error? Unpredictably, I should say. Why this is important to get into early on is because I think coverage is a super powerful tool. I'm obviously doing a video about it for that reason. But it's also easy to get in the trap of saying, oh, I'm 100% covered, you know, I must be good. But that's not really true. That's like a starting point. That's a really good, you know, goal to get to a really high coverage percentage doesn't have to be 100. It depends on your code base. And you know what the other developers think. But it's just a really good way to start off and say, okay, now my test runs on all of the different code. And now what do I do to make this even more bulletproof? I'll be touching on that here and there as we go through it to kind of elaborate on that definition. But I think you've got enough of that earful. Okay, so now we, as the developers can come in here and oh, I forgot to actually mention, you know, this is a really nice way to show, hey, you know, when we run this, more clues that we need to run armor is that there's this yellow highlight here, this is a conditional highlight. And it says branch not covered. That's how you'll know for conditional logic, that one of the options has not been covered. So the code comes in here, and it runs this type equal to weapon, and it's true. And so it runs this function, but we never run it when it's false. Same thing here. You know, we say, oh, hey, do you have rarity? If you don't, then use common. So we can see that we always pass in a rarity property when we're calling it. So now we can take a stab at what we need to run next. So let's go over and kind of update one of our tests.

So if we go over to the generate item test here, we don't really need this log right here for now. I'm gonna make a new test. And so this will say should create armor object. You know, since we haven't covered that at all. And then in here, I'm just gonna show you that you can actually just run the code. You don't even really have to have a test to get coverage, which is a good and a bad thing, I guess. So we're gonna generate item. And from here, we're gonna do type and we'll set that equal to armor. And then what I'm gonna do is I'm going to set the rarity equal to... Let's do rare. Save that. Go away. Now, let's run test with coverage one more time. Oh, that was just test, huh? test dash dash dash dash coverage. We'll add that into our script pretty soon. Well, you can see our numbers got a lot better there. Okay? So let's go back over to our browser and we're refresh. Now check this out. I'm gonna refresh kind of like, let's say like down here. Let's see what happens. Boom, look at all that. All of that cleared. So just by calling that function, we have run our code in a test against all of this other functionality. We can still see that we have some weak points. We didn't check the don't pass in something. What happens? We got to this interesting branch where we always get zero here. I guess we'll kind of come back to that. And then we come up here and it looks like we ran most of this code too. And notice that some of these are being run two times now because we have these two different tests. And so they're always kind of this is kind of like showing you, hey, what path is traversed most through your tests? And you can see this kind of makes sense. We generated a rarity epic item and we generated a rarity rare item, rarity rare. And then we did not do the else, which is actually a common. That's the fallback. And that's why when we go down here, you know, we don't have this test today either. Now this is kind of one, this is really cool, but it's also to really emphasize that gotcha here of coverage that notice that I just ran the test. I just ran this code. I didn't even do an expector or anything like that. So it's a test that the code, you know, completes successfully and doesn't have an error when you pass it in an armor type. But it doesn't really test that anything useful comes back, right? So let's quickly do that. You know, we'll just copy this and I'll go back up to here and I'm going to copy this expected object as well. And obviously this is not for, you know, learning how to test every single thing here. We're just going over coverage. So we're going to make it path of least resistance. So let's say we got an armor and we expect that to come back as a rare. Let's actually leave that epic so we could see it fail. And then this case, it wouldn't have damage. So we could go over to our report here and be like, okay, what does it give us back? We could look at the code too, of course. Generate base armor. It looks like it gives us back defense and an armor type. So let's say defense and that's going to be a number, an armor type. And that's going to be any string. And if you're curious, that's just right up here. You can see what it does is it goes to this sort of const type where we're going to be picking from one of these things randomly when we generate it out. Not super important. Just if you're curious to follow along, don't worry about that too much. So now this expected weapon, we're going to change this to expected armor. And then generate item armor, blah, blah, blah, blah, blah. Oh, this right here. We don't want to pass in weapon. We want to pass in, we need to store this. Armor equals armor. And there we go. So let's do that. Let's test this one more time. And you'll see we got that fail, right? Now notice that the test suite failed. It still printed out our coverage report. We didn't get any sort of coverage fail yet, but we did get this failure. So we know that that's working somewhat as intended. It's really not the most rigorous test yet, but we're getting there. Okay, so that passes. Now the thing is that none of this really should have changed. And so once again, I know I'm beating a horse here, but such a strange expression. I know that I'm beating this to death, but we, I just want to go over this last time here that the coverage is not changing, even though we wrote a better test. So the coverage led us to say, yes, we need to test this thing, but we need to actually make it a useful test still. Okay, done ranting. So now we, let's like take it one more step further here. Let's say, okay, we know that we're not running this, this else statement here on rarity. And we also know that we're not running the shortcut here, short circuit, and we're not doing a common, which we would get from passing in no rarity at all. There's also this little thing here. So let's like just do one more pass super fast. So we'll write a test now. And this test is going to say, should default to common item when no rarity passed. Something like that. Okay. That got a little bit long. Sorry, let me see if I can scroll down over here. Oh, the challenges of zoomed in. Okay, so in this case, what did we have here? Well, we could just generate an item with just a type. So let's do, I'll do generate item, and we'll pass it a type equal to weapon. And that's it. Let's test that. Okay. Run our test here. And that's looking better and better, more and more green. We love to see that. And there you go. Look, now common is covered. And then we come up here and this is covered. So we're getting closer and closer. You can see we're kind of whittling away at this file and getting a little bit more confidence each time. Now, I'm not going to waste more time by, you know, writing a proper check test on this, because you're kind of getting the point. But let's say, all right, well, let's do one more here. I want my template here. So let's say that I want to do my short circuit, and then we'll kind of call it good on this file. So I'll say, should return null when no values passed. Or actually, in this case, it was no type passed specifically. And this is, again, nice for us to read, you know, coming into the project is sort of a documentation of like, when does this, what does this function do when pass these things. So we'll generate item. And when we generate that item, let's see what happens if we just pass it nothing at all. Okay, see, that's interesting, because we tried to destructure this property type of undefined. So in this case, the cover just kind of led us to an actual problem with our test, because the person writing this was like, okay, well, I'll just pull off type by default, yada, yada, yada. And then I'll check the types not there. And that's really the only situation where I want to, you know, catch here. I worded that poorly, but I think you get the point. So let's see what we can do about that. Okay. So if we come back over to our generate item, of course, we could just make it, you know, succeed by passing it or an object that was empty, let's say, but that's not really necessarily what we want. You know, we want to fix things as we go. So to get this passing, let's do something like this. Let's say that I'm going to, you know, cut this out right here. And I'm going to change this to maybe let's say item options. And we're going to by default, use a object, we're going to, you know, fall back to passing in an object if they didn't pass in anything at all. And that will let us do something like this, we'll say pull off type and rarity from item options. There's of course many ways you could do this. And this is just a quick way to make our code just a little bit more robust here, because now, we can guarantee that at least type can be pulled off of this empty object as undefined at the very least. So now let's run our test. And you can see that it passes now. If we go back over to our coverage one more time, and we refresh, you can see that this has hit many times, of course, and we now successfully check this scenario, this unhappy path here. And that's really all that's left. We've covered a lot at this point. And we've I feel a lot better about this, I think that anyone doing this should, especially if you're just kind of coming in and looking at some legacy code and building a little test harness around it, we can see that we started off with very low coverage. And now we're covering a lot more situations. Again, we need to write a little bit more robust tests there. But at least we're checking that you know, it doesn't just straight up fail. But what's glaring at us right now is this little bucko right here. And this is something that I just wanted to illustrate really quickly. So if we take a look and hone in on this code, what it's doing is it's saying, okay, this base item that we're kind of creating, I know we haven't gone through the code that much. But this base item at some point is checking, it's going to add the stealth bonus property to it. And you can see that if the armor type is light, you're going to get a three bonus. Otherwise, you're going to get a zero. Now what's interesting about that, though, is this armor type itself, let's actually go back to our code to check this out. So this armor type right here, you can see that we come up here, when we generate base armor, armor type is sort of randomly created from this list up here. And so this is outside of the control for us at this point, because we can just run this test over and over and over. And essentially, you know, one of these times, it's going to switch on us, which is pretty confusing. Let me try to show that real quick. Let's actually add a log, though, while we're doing it. So we have this one generate armor case, right? So let's log out armor. So we have an armor that is the light type, which means that you get the bonus of three, right? And so now you can see it flipped here. So flip from three to zero. And just to show you one more time, if I run that again, we got light again. And now we got heavy this time, if you're not noticing, that's right here. And then when we flip over here, it's going to change every time. So this is kind of something to know here is that like, you know, the coverage gets a little trickier or tests in general get trickier when you have randomly generated things. And so at this point, what you would probably want to do is do some sort of mock the method that you're testing. And or you could make it to where you have overrides that you can pass in. So you can get some hard returns. hard coded returns. This is what I mean by that. And this gets a little bit too much into a side rant. But I'll just note that, you know, we're testing generate item only. And, you know, in our tests here, each of these tests is really just calling generate item. But we really, if we're doing unit tests, you know, we should be calling some of these individually to test them, and not relying on our test to kind of go through each of those things. So that's what I feel about it. So there's various different ways that you could cover this. I'm just showing that to you. And we're not going to solve that here, because it attracts too much from the main point of the video. But just know, this is something that coverage is a little bit tricky to track, but actually can be really helpful. Because I've had times where if I have some sort of random thing, all of a sudden, you know, my, my test coverage will be lower. And I have some stipulations to fail it. And from that, what will happen is it will reveal to me that my tests aren't actually that good, because they're kind of random, whether they cover something or don't or run one track or another. Okay, so now we've gone through this process of utilizing coverage to kind of hit to us what to test next. And I think that this is a really good tool, if you're newer to testing, to kind of introduce you to what you know, what really should I be testing. And like I said, also, it's really nice when you're coming into another code base, and you can get a good idea of how well covered all their code is. So, you know, I think it's pretty awesome. Hopefully, you do as well. But let's move on to the next part of coverage where we can make it even more powerful here. And that is to add a config option. Now, what I want to show first for that to kind of reveal that actually, let me remove my log super fast. I'm not flooded with that. We've used its purposes. And for now, we're going to be done with generate item test for a little bit. When we ran this test, this coverage report here, we noticed that this source utils helper JS was 100% covered here, right? Now, what's interesting about this, there's a couple things. When we go to utils, notice we have this get biggest change file in here, but that's not in our coverage report. Huh, that's interesting. And then if we go to the tests of the helper one, this is the other thing, we only test this get random int version of it. But if you go to helper, there's actually a get random int in range as well. Hmm. Well, let me start with this one. This get random int in range is actually being tested by generate item, because generate item imports it, let me go up here, and uses it. And so through writing the tests of generate item, you've indirectly tested get random int in range. This is good and bad. I mean, it's great, because you are still testing it in sort of a real use case scenario, but you're not really directly testing. The bigger problem to me, though, is this get biggest change is like not reporting to me that it's not tested. And that's because the default, which I think is a good default for jest and coverage is that it's only going to run without any additional configuration, mind you, it's only going to run against tests that exist. So this helper test JS imports, get random int and runs it on that. And because of that, because it's importing this file right here, it's going to put that into the coverage report. But get biggest change doesn't have any tests that run it at all directly or indirectly. And so it's just not going to be in our coverage report. And this is nice, because it's a good way to ease yourself into testing. But I want to be a little more strict. I really want to know out of all of my files, what is tested, what is not, what is covered, what is not. So in order to accomplish this, what we'll do is we'll make a jest dot config dot JS, you could also do this as a jest property in your package JSON, but I think this is the better way to do it personally. And then we'll do module dot exports. I think this can also be in a JSON file, whichever you prefer. And we're going to start with this property here. And it is going to be collect coverage from and that will be an array. And so what we'll do here is we'll say I want to test against source. And then I'll do star star slash star dot j s. And so what that's going to do for me here is I'm going to say, you know, I want to test anything in my source directory, because that's my actual code. And, you know, maybe I should show you just really quick. Let's say what if we didn't do that source? What if we just did star star dot j s? This is kind of interesting, huh? So notice this jest config dot j s was now tested. And all these different other things in the coverage folder were tested. So that's kind of the point that I'm getting at is that we don't want to test every single thing in our project, because some things are generated, some things are node modules, some things are etc. So what we want to do is go back to this point where we can say anything in my source directory. That's what I've written now so that, you know, you can also do something like this where you say, don't test this. There's a couple of different ways to do this. Don't do anything that has node modules in the name. I don't think you actually need to do this because I think by default it's going to ignore coverage. The ignore coverage setting is going to have node modules added to it. They show it a lot in the docs, and so it doesn't really harm anything. So I always throw it in there. Now, this also allows you to ignore, you know, specific files just so that you know, like if you wanted to ignore, let's say, get biggest change is something that we're not ready to run coverage test at all on right now. Well, you could throw that in the ignore if it's, you know, blocking you from moving forward for some reason. You'll see why it would be blocking you in just a moment, actually. So let's run coverage again now. We should get a much slimmer report. But now look at this. We have added get biggest change to the source utils when before it only had helper.js, and it's telling us that it is 0% covered. And look at this global. We used to be, so let's go up here a little bit. Look how good we used to look. Great 93.33 branch. That's fine. That's actually pretty good. Then down here, you know, that was that one conditional that we couldn't cover. And now it's so much lower because this file overall has dropped it. I really like this because it's a good way to kind of it's like a measure of confidence of, you know, of all my files. How well tested are they? Well, if some things are like completely not tested, they're really going to bring down your overall confidence rating in the code base.

Now, if we hop back over to our coverage report here, now when we come back here to the root, you'll see that this source utils, which used to be all green, now our source is actually green, but source utils is much more hit up. And if we click on that, you'll see that this is all the stuff that we haven't run any path whatsoever. Now, to speed this up a little bit, I'm going to, you know, toss in a couple of tests right here that I've pre-written just to show you that getting covered a little bit better. So I'm going to make a file real quick here for this in source utils. I'll write a new file and I'll just call it get biggest change dot test dot js. Paste this. So I've got a few test cases already in here now. And so as we can see, when we call them, all we're really doing is we're just saying, hey, call get biggest change with a thousand. In this case, it's copper pieces and the copper pieces are passed in and we say, OK, well, that should be exchanged to one platinum if we want the biggest change version. And then here, you know, you can see you get two platinum and three copper. And finally here, we're testing a big number like 40. So come back over to here, run our coverage again. We'll see that's looking a little bit better. It's yellow, right? Let's come over here and our coverage report and refresh. We're looking a lot better, right? It's kind of fun. So really, all we need to do is cover, you know, oh, hey, you didn't actually cover any scenarios where you give them back gold or where you give them back silver. And so I'll do that super fast. I'm just going to copy this test here. And I'm going to pass some silly scenarios. So let's say you pass in 100 or maybe 200. Doesn't really matter. That should give you some gold. Pass in another one. Let's do 70. Should give you seven silver. Run that. Nice. Getting better and better. You can see uncovered line is still line 11, which we'll come back to here. Perfect. But this line 11 where we didn't pass anything is the last thing. So let's go back up to the top. And normally I'd write another test just to kind of identify this a little bit better. But I think this is fine for now. So let's pass in nothing. Now, what we should expect to get back is just this object of no coins whatsoever. And oops, go back over here, run it. Ooh, look at all those hundreds. And there you go. Now we have this file fully covered. So let's show how we can make the impact of that a little bit more drastic, actually. This is where it gets really fun to me. So we've set up a config to where we're making it to where we actually know now, like if any code in our source folder whatsoever is not covered. And of course, we could expand that to your project's needs, however you like. But what if well, actually, look at this. We can see that it's the test, regardless of fails or not. We go all the way back up here. When this was terrible, the test still pass, which is good. That's what we want. But we're going to be wanting to add this into some continuous integration in a moment. And to where what the result of that is going to be is every time on GitHub, when a pull request is done into the main branch, for me, I'm going to run a coverage test. And if any tests fail, then you can't merge. And if the coverage falls below a certain threshold, then you can't merge. So let's set the first steps up for that. So in this just config, we're going to add on another property here. This property is going to be coverage threshold, not two H's. And that object here is really interesting. It's going to have a global property. This is the kind of the default way to set this up. And the global property here is really neat. What we can do is we can say, what global coverage do we expect to have? Well, we can target each thing. So we'll say for branches, I want 60. Let's say. Let's make it really high at first. So we'll say 90. Functions, 90. Lines, I guess we'll do the same thing for all of them for now. Now, you may be wondering, like, how would you come up with these values? I'll talk to that in just a second. Let me actually show you the impact of this. So right now, when we run it, we should be good. Because we've covered so many things. But check this out. What if we go back to this get biggest change test file here? And what I'm going to do, I'm going to wipe out all the rest of these tests. So I've commented them all out, and it only runs the one. Let's see what happens. Ah, interesting, eh? So now we get these right here, where it says global coverage threshold for statements 90% not met. 70.83 is what you got. And you can see that right here, confirming that. So what does this mean? What does this red text even do? Well, it's obviously alerting us, which is nice. But it's also going to return a sort of error exit code here that when we run it in our, in really any environment, but in particular for us, our continuous integration that we're about to have, it's going to fail when it doesn't meet this criteria. And that test will actually fail as a whole. So you're going to get a failure if your tests don't pass, which is obvious. But you're also going to get a failure if you don't have proper coverage. And this is to help protect your system or your project as it grows and grows and grows in scale. And people continue to add functionality and change functionality and all that stuff. If they're not adding tests as they do that, eventually you'll fall below this threshold and it'll fail you and be like, okay, it's time to write some tests, guys. So that leads me back to, you know, what I was saying before is like, okay, well, how do you come up with these numbers that we came up with here? How do you say, you know, what makes you decide 90, 90, 90, 90? It's really based on your preferences. If you have a new code base that you're trying to test slowly and build this test harness over, you know, maybe a legacy code base or something, you probably want to start lower and kind of increase the strictness as you go. If it's new, you should probably set it higher so that you're alerted really early. So let's go to another scenario here, actually, where, let me close this over here. We don't need that right now. So actually, what I'm going to do here is I'm going to bring all this back in, get us nice and covered, and I'll show you through a different type of example. So now we're passing. Okay. But let me show you a higher level override that we can do here for this next demo is we started with global. But what we can actually do is target individual folders and files how we like, which is really sweet. So in this case, I'm going to do dot slash source. And here I'll do utils. And so I'm going to say the utils folder is very important to my project because everything's using it. I want very high confidence there. 80, 90, not enough. So what I'll do is I'll just copy all of these lines here to override them. And I'm going to say 100. Got to be 100 everything for utils. Okay. So we'll come back here. We'll run this. And it's passing right now. So that's fantastic. We're very happy. But let me show you an interesting scenario. And then we're going to push this up, show some continuous integration. So let's say that if you recall earlier, there was this test here for helpers, and we're only testing one of the functions. Well, the other way that we were testing it was in generate item. This get random int in range was used. So what if someone, you know, in the future came in here and was like, you know what, I don't really like using it that way. I want to do get random int only. And that takes, well, actually, they could just leave it that way because it would just not pass in the second argument. So let's do that. And now you can see that we're not using this, right? So what's going to happen here? Well, when we run this coverage example, we're going to fail all of a sudden. And that's because we weren't directly testing this. And by this, I mean that that particular helper that we were indirectly testing before, and now we're not. Okay, so let's say someone did that. And now I'm going to create a new branch just to kind of show a normal kind of workflow example. And let's say, let's call it, you know, swap random int, something silly like that, right? Normally, I would review all my, you know, different changes and stuff like that in VS Code. But for demo purposes, to speed this up, let's just commit it and say swap random. What was it? Get random int in range out. I won't get more specific than that. Okay, so now we'll push this. We're going to push this up here. Now, I've already gone and set this up, but I will show you kind of what that looks like really briefly. If we go over to here, you'll notice that this is my repo. And of course, GitHub's awesome is going to say, oh, do you want to you got a new branch that you've pushed up? Do you want to make a pull request? Well, yes, I do. So I'm not going to bother doing anything fancy here, of course, but we'll speed this along. And now what's happening is these checks are running automatically. This is your continuous integration. And you can click this to see some details. Let me open a new one here. And this is really neat. So the test and coverage. I don't know why it took me to that weird screen. We're about to run this and then we can see that it runs. Yada, yada, yada. And in this case, it passes. OK, so everything ran. Let me let me dig into that just so you can see. Everything passes, and that's because right now we're not we're just running a the just CI, but we're not doing our coverage yet. So I want to show you that when we push this up, even though it has failed our coverage, it's just passing the test. And that's all we required in order to merge. So if we go back over here, we'll see. Yeah, we're good. So let me show you that real quick by switching it. If I come back to here to my code and what I'll do is in the package JSON, I'm going to add a dash dash coverage flag here now to the test CI. So you don't have to really worry about this flag. It's mostly just for snapshots, the way that they run and see I to be a little bit more efficient. It's pretty standard for just and then the dash dash coverage here, obviously, is now we're going to be like, OK, well, I actually care about coverage as well. So let's add that. Add coverage requirement in CI. Check that. Push it up. And let's see what happens. So this is the this is that one run. So we need to back out of that. We'll see that this has started up again. OK, go on over to this. See, it runs checkout code diving into this part right here. Just follow it along. Error. It's kind of cool, though, you could see it did actually print out our little table here of all that stuff with the coverage. And it gave us these comments on why it wasn't met. But eventually we got this. This is what I was talking about before this exit code one. So when you get that, it's actually going to fail this step entirely. Notice now we have this red X. All checks have failed. So in my case, for my repo, and this is pretty common for any public repo, the administrator can override that if they want to. But even if you go to it's really nice, it's going to make you check this box. It's like, you're really sure about that. So cancel that here. And actually, if you want to see, I probably should be covering this a lot more in a CI particular video, but I'm sure some people will be curious. So I'll just briefly show you if you pop on over to your your settings in your repo. This has to be a public one to do this. If it's private, you have to pay for specific, you know, access to get bumped up. But you go to branches right here. You can see that there's this branch protection rules here. And if I go to edit, all I'm doing here is I named the branch main. And I'm saying I require these status checks to pass before merging test and coverage, because normally it'll still show up in the CI like so. It'll fail here, but it won't require it to pass to merge. It'll just kind of be like, hey, just so you know, this failed. You know, you could still merge willy nilly. So this nice little branch protection rule is saying like people can't even merge without admin access to this branch. If any checks fail, that's how serious you want to be about it. OK, so let's fix it real quick. So we'll go over to here again. And back to our files. And so we have this helper's test. So really what we want here, this this is kind of cool because the coverage has sort of shown a vulnerability in the way that we're writing our tests. And that's that we are indirectly testing through generate item one of these helpers. And we we can now be like, well, you know, I don't necessarily want to change the code base of generate item to use this anymore. You know, really, we delete this, of course, and we're just not going to use it maybe for now. Maybe you use it somewhere else. Who knows? But for this to not happen again, and for someone to know that they can confidently use this method in the future, we should probably write a test specifically for it. So that test is going to look, you know, something like this. Pretty much the same thing, but, you know, we'll call it between four and nine. We'll say that it should equal or be greater than four and should be less than nine. We'll run that. And this is what's really nice about CI is a lot of times you'll forget. In this case, we could actually just run NPM run test on CI since we have that script now. Oh, we got an issue here. Ah, yes. We're calling the wrong function. Well, that's always nice to get. Right. So we actually need to say import int in range. Good to know that that can fail, right? That's the nice thing about testing. And there you go. We could run it again and again if we want, but there we go. So what I was kind of saying was that what's nice about CI or continuous integration in this case, for this context, is that, you know, people are not going to remember to run the tests every single time they change code. They're just not. So this is a way to sort of enforce that. You could do it on the push level, every single push. In my case, it's been sufficient for me to say, look, you can't merge into the main branch until you pass these tests. So maybe you would do that on feature branches into any merges. It kind of depends on how complex your project is. So with that, now we'll show the passing pipeline. So let's say added add test. Proper grammar here for git commits add test for git random int in range. I don't know why I care about these messages. It's just an example. So we'll push that up. And because we have this PR open, it's automatically going to rerun. Sometimes this doesn't update right away, so you just refresh. I took a while. I took a couple of refreshes there. So here's our old failed one. I'll just leave that open. Now we can go over to here. These are pretty quick, usually. If you've got a lot of stuff going on, it can take longer. And there you go. And look, this was automatically swapped back. And now we can successfully merge this into our code base. So I can click that. I can say confirm merge. And there you go. It's really neat that you get to see the history here, too. It's like, oh yeah, at one point this failed. And then all your checks succeeded. Now if you're wondering how that works in particular, like I said, I'll probably be doing a continuous integration video if you guys want that and want to see more of the ways that you can do that in your project. Just let me know in the comments or on Twitter or whatever that you want to see more of that stuff. But just so you can kind of get a quick peek here, if you have this.github workflows, obviously this is on GitHub. It's different from Bitbucket or GitLab, but you have this folder structure and you can look it up on GitHub Actions. I just have this test.yaml file. And I'm saying on pull request on these branches. So any pull request to main, I want you to run this test coverage. And here's just the name of it. You don't need to worry about running on Ubuntu latest, checking out the code. It's a pretty standard step. But this is the part that we care about. We run npm ci, which is sort of npm install, but on the ci, a little bit leaner. And then npm run test ci. And it's that simple. So I'm just telling it to, you know, install my packages and then run this command that I have. And just with that little bit of setup, we have some really awesome configuration to protect us and protect our code base and let us know what's going on and let us know when people change features. You know, what's really sweet about that is in that scenario that I just showed with generate item where I swapped out this value, this could be a PR that someone's making to my code base, whether it's open source, whether it's on a team. And I wouldn't have to dig through and find that that thing wasn't used or is used differently. I would immediately know from that test failing that, oh, we actually don't have any tests that actually test this helper out anymore. We're relying on that. So it's really awesome way to automate and keep yourself informed and keep the confidence in your project high. So that was a lot. But if you made it through all that, hopefully you're wiser for it. I think I've spoken to the benefits of the coverage utility function and coverage reports in general enough that I don't need to re go over them again here. I hope that was fun and interesting for everyone who stuck it through that. And I encourage all of you to try it out in your own projects and see how it makes you feel better about your code and what things you discover. I've learned a lot by using just coverage and tests in general about the soundness of my code. And that's why I'm a big believer of testing and unit tests and all the types of tests in general now. And with that, I will say goodbye and hopefully see you in the next video.