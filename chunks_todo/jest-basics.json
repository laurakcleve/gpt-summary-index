[{"text": "Hello and welcome to Swashbuckling with Code. I'm Jimmy Cleveland, and we're going to go over just the JavaScript testing framework in this video and see what it has to offer. If you're not familiar with testing in JavaScript or unit testing in general, what it allows you to do is write some code that effectively runs whenever you run the script, and it tests each of the individual units of your code that you wish to test for reliability and consistent results. And if you're wondering when you would actually run those test scripts, well, it's actually up to you, but it's really common nowadays to run them in continuous integration setups or in your code pipelines or anything like that, where each time that you push up your code or you make a pull request or anything like that, some tests will automatically run, and it'll give you that extra bit of confidence that you didn't make any mistakes in those commits and those pushes or PRs, etc. You can also run them manually as you're doing things, and you can even do a really cool thing in Jest where you have a watch mode where it's watching your code for file changes and automatically running the test on those codes so you'll get instantaneous feedback of whether you've introduced an error or regression or anything like that. This is really useful for you to get some extra confidence from running the tests as you run your code, but I think it's even more useful for maintainers or people that are foreign to your code, people reviewing your code if you work at an organization or if you're doing open source, or even users of your code to know that your code runs as expected by just reviewing the tests or running the tests really quickly. It's kind of become a standard in a lot of companies that your repository or project must have a test suite. This even allows people that don't write the language of code that you write to run the tests and see that it does run as you expect it at least. It even doubles as an extra form of documentation, which I think is really cool, and I go over a little bit in the video where as you're writing these tests, it's showing to someone who wants to come in and look at your code. Rather than looking through that code and having to mentally load everything that's going on, they can start at looking at, well, how do you call this function and what do you typically pass to it and what does it typically return? So with those definitions out of the way, I'm going to show you setting up a project from scratch and adding Jest to it, which is pretty simple. And then we're going to add some tests step by step as my usual format, and I'll show you what it's like to pass a test and fail a test and making some new files so we can bring into test a little bit of that red-green refactor pattern for test-driven development. If you've ever heard of that, I'll just briefly touch on that. We'll also go over some of the nice little tools and functionality, such as the watch mode, testing regular expression patterns for files, matching, and stuff like that. We're going to go over coverage reports, which is a really, really cool feature of Jest that can show you how much of your code is actually covered by your tests. And it'll give you a nice little visual report of what things are missing, what branches of code that you're not actually checking in your test, which is awesome. And then I'm going to wrap it all up with snapshots, which is one of the things Jest is really known for. Love it or hate it, it's a hot topic. But it's actually really useful in my opinion, and I'll just show you some basic use case of it in a little bit more complex testing example. And I think that sums up the overview, so let's write some tests. [\u266a Music \u266a All right, so let's start out by making our project. And what we'll do, I've already made the directory here, but we'll do npm init dash y. And I'm just going to answer all the questions yes. And then I'm going to npm i for install Jest. And you can obviously use yarn if you prefer, doesn't really matter to me. Now that that's completed, we could make a script in our package.json, or we could just run npx Jest. Now when you run that, it's going to run all the tests once. And in this case, you can see it says no tests found. So what we'll do is we'll do npx Jest, and then you can pass a dash dash watch flag to it, which also isn't going to work for right now. And they tell you why, it's because without git or hg, please use the dash dash watch all command, which is nice of them to give you that hint. Now the reason for this is because the watch all command, just every single file change runs all of the tests. Whereas if you're, let's say you use git, it will watch the changes to each file and try to run the tests on only the files that have changed, just so you know. We're not going to add git for now, because I don't really need it for this demo. So we'll just continue to do watch all. So we'll say npx Jest dash dash watch all. All right, so you can see that it didn't find any tests still, of course. And we have all these little helpers, which I'll go into a few of them a little bit later. For now, let's actually make a test to satisfy that. So I'm going to make this woof.test.js. And you can see that it automatically picked it up, and it says your test suite must contain at least one test. So dialing back just a second, this woof.test.js, well, I named it woof because I'm actually going to have a function and a file called woof. And that's a really common convention for testing is to name the test the same as the file and or the function. And then there's a few different ways that you can structure your code. For now, we're going to keep it flat, but it's sometimes common to put all of your tests in a test folder. And it's sometimes common to put them right next to the files. It's really up to you and your workflow to decide what's best for you. I kind of like the merits of both, but I typically put them right next to my files. For now, we're going to keep this flat. And you'll notice that this.test, well, this is why it's picking up this file automatically without any configuration at all. Just is going to look for any files that are named.test. And here, I just want to show that there's also a common convention to call it spec.spec. And in some languages, that's how they commonly do it. But I usually see it.test, and that's what I stick to. So you can choose that as well, but you'll see that floating around just so you know. Okay, so now it says your test suite must contain at least one test. Well, they don't really help you out too much here, but you could read the docs really quickly to figure this out. So what you can do is you can type test here. Now, when we type test, this is actually a global that just is adding to the test runner. So when you're running the test suite, you have access to this and any of these like test files. So I'll just call that function, because I know that I need that. And you can see that it says invalid first argument, undefined, it must be a string. So that's pretty cool. So let's do that. We'll say our first test, we'll save that. And now it says missing second argument. It must be a callback function. You'll notice they also make this little hint to perhaps you want to use test.todo, which is really cool little helper that I'll show you in a little bit. So for now, let's give it a function. Now we can just pass it our standard function format, just like this, or you could use an arrow function. And I'm not going to do anything in that I'm just going to add the braces and save. And you can see that our test passes. So this is interesting, because we haven't actually done anything. But what it's just looking for is that the code runs successfully within this function. So you might wonder, okay, well, what causes the test to fail? Well, let me show you. You could just type some gibberish in here. And that's not real code. So it's going to fail, right? You could also type, let's see, you could actually throw an error here and say did not work. And now it'll actually give you that message. Our first test, you can see the X failed, did not work. And something interesting that I want to show you is this, unlike, you know, having all of your code execute in like one file, where if you end up running an error like that, the rest of your code will stop executing in JavaScript there. Well, I can actually make another test here. And in this test, I'm going to say failing above or something like that. There's really no rhyme or reason to what I'm doing. And you can see that that test still ran. And that's the thing that I wanted to point out is this is kind of an isolation here. Whenever whatever's running in here fails, it's still going to go on to run the rest of the test suite because you want to know like in total how many tests fail. You don't want it to just fail on the first one. So that's really cool. And I just wanted to show you that's basically how that works. So next what we want to do is actually make a file for this. So I'm going to remove this and I'm going to remove this for now. Save both of those, get our test passing. And then we'll make a file called woof.js here. And in this file, I'm going to make a function. I'll call it woof. And I'm actually going to make it take a string. And what I'll do is I'll just return the string and then I'm going to change it just so it actually has some sort of functionality. And I'm going to add a loud woof to the end of it. And I'm going to use module.exports here to export that woof. Now, of course, you can use ESM, Eclipse script modules if you have that set up. But we're just going to keep it with this module exports, js format. So let's actually import that now. So I'm going to say const woof equals require. And then I'm going to go grab that package real quick. Woof. And now importing it works fine. And then here we'll actually run it. So I'm just going to call woof right here straight up. And I'm going to call it with a string ohero. Now, you can see it's still passing. So how do we know that it's actually running that code? Well, if we hop on over to woof.js, let's just throw a console login here. And then let's say the woof ran. And then we'll just put, like, the argument that it got here just to show that. And this is pretty neat. So you'll see over the left here there's this console.log the woof ran ohero. And that's what we got. All right. Well, that's pretty neat. And so you can see that it's actually running our code from the test suite and receiving our argument. Now, a quick little thing that I want to show you here is I'll close this down. Now, there are times when you're troubleshooting and you want to console log some stuff, especially when you get into mocks or you're just trying to figure out, like, how that all works. And it's really flooding up your test logs. Well, what you can do, just so that you know, is you can run, let's say we run this npx jest again, just to show that you're going to see that console log.", "title": "JavaScript Testing Basics with Jest", "url": "https://www.youtube.com/watch?v=__QEPUdnJS0"}, {"text": "If we run jest dash dash silent, it actually won't print out any of the logs that it receives, which is pretty cool. So that's just a nice little flag to know in case you don't want to be adding and removing logs over and over again, just to hide that stuff and see the tests. So let's go back to our watch mode. Watch all here. So let's actually make this fail now from the actual code that runs. So I'm going to take out that log and I'm going to change my function. And let's say what I actually wanted to do is take the string that comes in and do a dot length. OK, and so now I'll have however many letters there are in my string. OK, and if we want to see that, we could do let's store the result for a moment. So result or whatever you want to name it. And then we'll just console.log right here. What are we getting back from that result? OK, you can say we get eight. Woof. So what I'll do next is I'll pass it nothing. And you can see that when we pass it nothing, we get a failure, and that's because we try to read dot length. So it says cannot read property length of undefined. You get your typical error printed out. All right, well, let's just make that pass. Now we just go over to our code and then let's do a condition here. And we'll say, you know, type of string that's passed in the argument is not equal. To a string. If that's not true, then we want to return. OK, and then now you can see we get back undefined, but our test does indeed pass. So right here, this is a pretty normal workflow for testing. And this is what can help you get a little bit more confidence from your code. And why testing, especially unit testing in this case, is so useful, because you get this immediate feedback from running your function and then trying it out in different ways. And every time you find a way in which it could fail, you can write a test for that and then you can make it pass afterward. And that is known as this red green refactor pattern, which you might hear sometimes, especially in test driven development. So that means that red is a fail. You want to write your test first with the idea of what you want to test. And so that's our wolf function. And we'd say that we expect it returns the number of wolves or something like that. So let's do that. Let's say this should return number of wolves or something, you know, name it, whatever makes sense to you for your functionality. This is obviously a silly example. But from there, we'll make it fail first by passing it no argument like we did. And then we go into here and then we rectify that. And then now we have a test to give people some confidence that that works. And then what's cool about that is if someone in the future is not even looking at the test files or anything like that, they come in and they change this for some reason. Maybe, you know, they change it to just look for was string passed in, but then maybe it's not. But then like something that an argument is passed in that doesn't have the dot length property. So they'll get a fail as soon as they run the test suite and they can know that they need to go back and fix that code. And so we get a little bit of extra assurance in an automated way that takes, you know, human error out of the picture. Now, this topic gets pretty deep. I might actually do a video covering all about unit tests and the why and the how and all that stuff, you know, not from an actual code perspective. But instead of derailing us, let's stick to, you know, just functionality for now. I just want to give you a quick little reason on, you know, why we're doing what we're doing. So going back to wolf.test, we can actually take it a step further here. I'm going to stop logging this for a moment. We can use the next type of jest functionality, which is an expect statement. So this will return what's known as an expectation object. And so what we can do is we can pass in the result there. You could also just run the code right there if you want to. So we could just call wolf like this. But I'm just going to do the result since we have it. And I like to store it in a variable because then I can, you know, quickly log it out or do other things with it. So that's just my preference. So when we expect result, that's not really going to do anything just yet that's useful for us. We need to add what is called a matcher after that. And then one of the most common matchers is to be OK. So when we run this to be here, what we want to say is, what do you expect to come back from this? Well, we know that from our previous log, we're going to get a wolf back. Right. So I'll just put that exact statement. Now we can see that our test failed because we didn't pass anything in. Right. So let's go back and pass in what was it? Oh, her role. And that gave us our path. Now, how do we know that that actually works? There's a lot of ways we could go about this. So you could just change the string. That's the first thing I do. And you'll see that it gives you this nice highlighter and it shows exactly what's different here. So maybe we got the number right, but for some reason we messed up the string and you'll see it's like, oh, expected received. Let's take a look at that real quick. So it tells you the name of the test that failed should return number of wolves and then the expect received to be expected. And it's trying to tell you that the green here is the good scenario. This is what you want. You think that you want eight WF to come back in this instance. And then received here is what you actually got. We have these kind of reversed in this, in this instance, but it's just for showing like how to make it fail. And so this, this is showing you here, okay, like you got something different and we're, we're kind of color coding it red. Sometimes this gets confusing in certain scenarios of why it's red and why it's green. So I always just kind of like default my brain back to, well, the expected is green. Like what we expect in our test should be green. So let's make that go back to passing. Okay. So now we have a passing test. We have a test that says, uh, here you go. You call this function and you pass it the string. And then I expect the result of that to be, and you can see how it kind of reads, uh, like a little bit like English here. And as this eight wolf as the argument. So that's an exact match there. Now, what's really cool about this is that this starts to show how testing itself can be used as a form of documentation. And I'm not saying that it should replace, you know, actual handwritten, uh, full explanation documentation, but it's actually really nice to come in and look at. Okay. You know, I've got this wolf.test file, so I'm new. Imagine we're new to this code base and we're trying to figure out how something works, or we've been working in it and someone added some new functionality. We're a maintainer. Well, I might look at this wolf.js and in this case, it's really easy to read, right? No big deal. But if it's this big, long file, it will take me a while to mentally parse that. But if I go into the tests, not only can I run the tests and be like, does this code even run first of all, which you should do, um, you know, run as they expect. The user who wrote it is like, I expect it to work in these situations and they write the test for that. But you can also come in here and read this statement and go, okay. So it looks like wolf is supposed to return a number of wolves and, uh, then we expect to be, oh, okay. So this is how you use it. Basically, you're saying, oh, you just, you get to see it in action. It's called. So you get a little example here and then you get to see what you, what the result is. And so that's a really nice way to kind of be introduced into a code base, uh, to see all the code and how they intend it to be run. So we have this statement and to re iterate over the terminology here, uh, this is called an expectation object that's returned from this. And this is called a matcher by just documentation. Now there are other forms of matchers as well. So let me show you, um, if you, you can run more tests in this one test, so we can expect result dot to be. And in this case, uh, let me show you the opposite. You can do a not in front of it. So you can say dot not in front of any of the matchers. As far as I know, I've never seen one that you couldn't do it in front of, and this will say, I, it's kind of how you say, I want the opposite. So I expect this not to be, now this is a dumb example, but you know, let's say bark and that works out right now. I could do a wolf just to show you. And it will fail. So the first statement actually passed because it did expect it to be, but the whole test fails if any of these expect statements fail. So in this case, let's say that, um, you know, we, we removed that. And the first thing that we wanted to test was, um, you know, we had an error one time where the number was printed out and we, we changed our code in the future that we didn't want the number printed out or for whatever reason. We might throw this in as kind of a cover for that to make sure that doesn't ever revert back to that functionality. Or we might want to just test the other values that we know might come up are not what is the result here. And so that's what the not is for in a lot of situations. There's other, other reasons to use it, but just to show you that. So there's also other managers here. So there is, uh, let's, let's wipe this whole thing out. I want to show you that there's a two equal, um, that sometimes people get confused with. And so you might wonder what's the difference between to be and to equal let's copy that and replace it. So now they say the same thing to be into equal. You see, they both pass. And that's what kind of trips people up. Understandably is that it's like, well, what's the point of this? Well, to equal is actually, uh, meant for objects and arrays for the documentation. And that has something to do with, uh, to be being more of a strict equality comparison, uh, or an exact equality comparison. Whereas to equal like objects and arrays, you typically have to handle them differently. Uh, you, you serialize them or something like that, uh, to do a comparison. So that's what two equals typically for. And let me show you that, uh, with a few examples.", "title": "JavaScript Testing Basics with Jest", "url": "https://www.youtube.com/watch?v=__QEPUdnJS0"}, {"text": "While I show you that you can actually, uh, just write, you know, whatever you want in here, first of all. So let's just make an object. Okay. And this will be an interesting example. So we'll say, uh, we have this object and we'll do a is equal to one. And then we'll just do two properties. Okay. B is equal to two. Let's actually get rid of that sidebar real quick here, just so you can see the whole code. Okay. So, uh, now of course that's not going to work. So what I'm gonna do is I'm going to copy that whole thing. We'll put it in here. So now I have, uh, both of the same thing, uh, being compared here and you can see that it's passing them. And if I were to change anything in this, it's going to fail and we'll actually give you this nice little layout here where it shows you, you know, the properties and exactly the difference between them. So remember that the green is the expected. So our two equal here is what we expect. And then the red is what we actually got that was wrong in this case. So let's go back to that and let me show you if we were to do two B here, it will actually fail. Now this kind of threw me off a little bit. Um, since it says, uh, if it should pass with deep equality, replace two B with two strict equal, they don't give you just the simple, the answer that it should just be two equal. Uh, but I never used two strict equal for any sort of deep equality check, to be honest, because I always test my objects in smaller chunks or snapshots or something like that. But, you know, if you happen to know why, uh, that is the error that's thrown there, go ahead and throw a comment. Um, but I'm not sure. And I know that it's just supposed to be two equal, uh, per the docs. If you're just doing a simple comparison here, and this is also true with arrays. So we could make an array right here and it'll be the exact same thing. Let me show you another common matcher here real quick. So we can also say expect results. And if we want to get a little more generic, which I find to be really useful, and I use this quite often, we can use to match here. Now to match, we'll also take a string, but what we can pass it is a regular expression, which is what I usually use it for. And I'm typically going to make it a case insensitive here. But what we can do is we could just pass, you know, a woof or whatever. Now notice I'm leaving off the exclamation point here. And when I test this, it actually passes. Well, what if I do something like that? No, it's not because that's not the correct format, but we could leave off the eight as well. So what this is essentially doing is it's saying, you know, can we match any part of this here? And so what might be useful in this example is we might want to use, uh, the escape D for like digits here and say, uh, I just want to make sure there's a number in front of it. In front of it. Okay. And then it has the woof afterward. So let's say that we don't do that anymore. You can see we'll get a fail, uh, here. Oh, let's actually go back and, um, comment this one out for a moment. And then you can see expected pattern. It'll tell you that the pattern that it expected and then what you got back, it can't give you, um, you know, as much nice highlighting, cause there's a bunch of different variations that could match this. But just to show you that, uh, that, that is a really common way to get, uh, the general meaning of what your test is supposed to do is to use regular expression matches. And I usually use case insensitivity. It depends on, uh, you know, what your use cases, you don't have to do that. Of course. Okay. So cool. Now we've got, uh, to be, to match. Um, those are some quick ones that, uh, I would typically use in this situation. So quick note for you following along here. Let's say that, uh, you start writing the statement out and when you're doing this, you don't get what I'm getting here. This IntelliSense or helper, uh, text here that will auto, uh, complete what you're typing as you type it out or list out all of the possible matches and all that stuff. So I had this, uh, once and what I actually did was let me show you, I installed the, uh, at types slash jest package to get the TypeScript types for it. And that fixed it for me. And what's really interesting is even though I installed it just locally in one project, it, um, has seemed to carry over, uh, to some of my other projects. And I'm not sure why that is exactly. I'll just do that real quick, just so that you can see. Um, but you know, that, that worked for me and then that made auto completion just start working. So just so you know, now I'll give you one, one more quick matcher here, just so you can see a little bit different than just checking for equality here. You can also do stuff like, let's say we have an array here that our thing returns and it has an A, a B, a C, something boring. And then what we can do is we can say to contain, this is a really useful one, and you can just check for a single property in that. So it'll test that. So does this contain an X per se? No, it does not. Expected the value X, receive the array ABC. So it's really nice. And so here, this is a, another way where you can just check like partial, uh, you know, maybe you, you give it, uh, some sort of object or array, and then afterward you expect it to just return back something that at least has one of those things. So that's a way that you can just kind of test for just partial functionality or partial properties or something like that. Okay. So now at this point, let me show you a little bit more, uh, usual way to handle this. What we're going to do is we'll close this down. We're going to go to package JSON, and I'm going to add this test script. So that usually comes with one, so you can just update this and you just simply need to run Jest. And so that will make it to where, uh, instead of running NPX Jest or whatever, we can run NPM run test, and then it will run our test. Pretty cool. Right? And what we can also do just as a shortcut, just so you know, is you can, if you're using NPM, you can do NPMT. I don't believe Yarn has that one, unfortunately, but you can just run Yarn test. So now what I'll do is I'll also get init just to show that. So now we've initialized this. And of course, this is mad because we have a ton of changes. So let's make a new file. We'll make a get ignore real quick and add node modules to that get ignore. So now we have get set up. Please go away. And when we now run NPMT, we can use the dash dash, a little argument here to append more arguments to our test script. So we can append more arguments to our NPM scripts and the dash dash watch. So that is going to be the same, you know, as if we were typing NPX Jest dash dash watch in this case. If you're using Yarn, you don't have to append that extra dash dash just so you know. So NPMT dash dash dash watch. And you could also make a script for this, but we'll just see if that works. And you can see that it now works. And what's cool about this, there's an extra little command here that you don't get with watch all. That's press A to run all tests. And I use this a lot. That's why I'm mentioning it. Because sometimes I feel like Git can kind of or it can make some mistakes when it's reading Git and not read all the files that it should be or not test all the files that it should be. And so I'll press A just to like give myself a sanity check. Like, hey, run all the tests again. I want to make sure that they all work. Now that you have the gist of Jest, let's go over coverage because that is really cool and something that I love a lot about Jest. So if you do NPMT, just for your, you know, Jest command, and then you can pass it a dash dash coverage flag. And what that will do, this is kind of an ugly format when it's too small. Let's make that more normal here. Rerun that just so you can see it. Okay, so now you have this table laid out. That's just because my zoom is high for recording. Typically that won't be an issue for you. So when we run this, you can see that it gives us this overall coverage of all of our test files. And what that means is it runs each of the files and notice it'll tell you the file itself, not the test, the wolf.js file where the function comes from. And it's going to say 75% of statements are tested, 50% of branches, which is like conditional logic, you know, if statements and ternaries and such like that, and then 100% of functions. So we've tested, there's only one function and we've tested that one. So it's 100%. And then how many lines of code? Well, that ends up being the same as statements in this case. And then it tells you the uncovered lines, which is really cool. Now, this is something that's really exciting to me. And it really leveled me up when I figured out how to utilize this. So let me show you. If you go here now, there will be a coverage folder in your directory that's been created. You could open that. You can go to this lcoverport and what you're looking for is the index.html here. Now I'm going to do reveal and explore just to open it. However you want to get to it on your file system, that's fine. So I'll open this up and you'll see here, let me bump that up for you real quick. That should be good. So you see here that it's kind of the same information, but it's done it in this nice little graphical way for us. It gives us this bar of this file to show all of the same stats that we saw before, a little bit extra actually. So when we click this, it will take us to the code for that file. And it's really nice. It'll tell you how many times each of these lines were run. And in this case, this red line is saying this is the part that is uncovered. If we hover over this eye, it's really small, unfortunately, but it says if path not taken. And so this is using Istanbul under the hood, if you're familiar with that at all. It's really cool. And what it's telling you is that this condition right here, you're not testing this. You are testing that your function runs and you've tested all this other stuff, the module.exports, it's just because you imported it. But this was never tested. You remember that we wrote a test before where we passed it nothing and that worked. So let's try that out again. So now let's say...", "title": "JavaScript Testing Basics with Jest", "url": "https://www.youtube.com/watch?v=__QEPUdnJS0"}, {"text": "We're going to write a new test and here we'll say, should return, and I'm actually going to make it do null. So we get something here, but I'll set that up first. Should return null when given, when not given a string. So we'll do a function this time. I'll just do an arrow function just to mix it up a little bit. So we'll do the same thing. We'll say const result, because I want to store it here, equals. Woof, calling it, and then we're going to do... Oh, we're calling it with nothing. That's right. And let's just do that. It's not even write an expect statement right now. So we can go back over to our terminal. We can run our coverage command again. And you'll see, you can just see from here that it passes. So you don't have to open it. But if you want a little bit of extra information, let's go back to here. This is all updated now. You can just refresh once you've opened it once. It's really neat. I kind of wish I had a sort of server or an auto open. I tried to look that up, but I couldn't find anything like that. If anyone knows of anything that allows it to do that, that'd be pretty sweet. You can append like an open command, but it's not universal across operating systems, unfortunately. So I didn't go down that route. You might write your own. But I wish there was a more standard way to do that. Anyways, rant aside, now we can see that all of our code, it's not highlighted red. We don't have any notifications or anything like that. And it shows that this has been run one time. And this is see how this has been bumped up to two, because we've hit this actual statement twice now because we have two different tests. It's pretty cool, huh? So now we can move back to our code and then do our test as usual. So we probably want to do in this case, expect the result and then to be null. This is one that you'll commonly do and not for like not to be null because you want to make sure that it doesn't or whatever. And in this case, that's what I'm going to do, because I'm going to show that it will fail right now. So we can use coverage to test that. Notice you'll still get the failures when you run coverage. Now, why would you run coverage all the time? It actually takes a lot longer. I think the documentation says that it can increase it twofold for how long it takes because it has to build all the coverage stuff. And so you wouldn't want to run this all the time. You probably run this in your continuous integration pipeline if you're doing that. That's what I do. And it's really nice for just doing some coverage text real quick. But just so you know, you could do that or you can just do MPMT like you normally would. And you'll see that this test is failing. So I just want to show that that is the coverage report also will print out failed tests and that the whole thing will return a failure code. So here you can see it expected it to be null when not given a string and it received undefined. OK, so let's fix that real quick. Go back over here and we'll just do this null. And let's say, yeah, that's that's actually what I wanted for whatever reason. I don't want it to just be whatever I want to specifically be null. Test that. Boom. We're passing again. And just to make sure that's our coverage. Awesome. So now we have that covered. Now, because this comes up so much, I'm going to cover this here, though it can get pretty deep, so I'm only going to briefly dive into it. I want to show that in a lot of situations, you actually would just want to throw here in your code. Let's say that's what you want. And you just want to say must be a string. You know, you probably want to write a better message than that. But in that case, let's go back to here or run T. And let me do the side by side again. And then let's start our watcher back up. And if you're doing this a lot, you might want to make a a little script for that. Just a reminder. OK, so now we get the error printed out. And I want to show how you do this, because it's a little bit different than the normal expectation object matcher paradigm here. So this time we don't want to be null. What we can actually do for this, I'll just do it all in one. We can say expect. And then you want to wrap this in a function and call. It's a little bit weird. And then now we can say to throw. And you could check for a specific error as well. I'm just showing that that passes right now. But you could actually say to throw. Well, did it throw a wolf? No, that shouldn't work. Right. Received message must be a string. So we could check that. Can we just pass? Let's try a lowercase one just to make sure. Nope. Must. There you go. So you can see that you don't actually have to put the whole error. You might just want maybe you wanted like something printed out, an ID or part of a message or something like that. You just want to be like, OK, it should throw roughly this error. Essentially, you can play around with that. Now, coming back to this, this is a little bit weird. You have to wrap this in a function here. And the reason for that is that it needs to capture it inside that function. Otherwise, you remember how we when we did just a straight up throw error here before? Well, that's what's going to happen. It's going to throw that error and it's going to fail the test. And so you can think of this as putting it inside of function here. We'll allow this expect to capture this, and then you can just check whether that threw inside there. And then we'll come back here and just, you know, you could do your coverage again and everything's going to be that is very unfortunate that that's so hard to read. OK, so now you can see our statements are only 80 here. And so let's go back. To this refresh and you'll see now we have this return null because we didn't we can't return anything when we throw an error. So this is pretty cool, just kind of showing you that you might leave some dead code here that isn't tested for. And so it'll catch more than just errors. It'll catch kind of just little mistakes and stuff like that. Not necessarily things that just, you know, fail when you throw an error. When you run it. So let's quickly go over, get rid of that, and then we'll be good to go. Now, I don't run coverage all the time. This is typically something that you run in your continuous integration suite or, you know, every so often when you push up or something like that, it's just a good general check to do, especially when you're writing a bunch of tests. Now, there's because there is some really cool stuff that you can do there. I'll probably do a whole video on coverage in general, because there's a lot to cover there. So let's move on, because there is some other cool stuff that we want to get to now. OK, so back to running our test suite here. Let's say that we know that we want to write another test, but we don't have the time right now. This is a really neat thing that we can do. We can write test and then dot to do. And with that, we could say maybe we are going to test for should not allow numbers to be passed or something like that. You know, whatever that ends up being that you think about. And you'll get this. It'll it'll pass everything like normal, but you'll get this little. It's supposed to be a pencil, I think. This should to do should not allow numbers to be passed. And that is really, really neat. This is a little bit better to me personally than writing comments, saying write a test for this or write a test for that, because it does actually get print out, printed out in your test suite when it runs. And you can see it actually right here in this nice little format. I think that's just a cool little functionality that they added. OK, so what else can you do here? Just give you a couple of extras, a little fun ones. Let's say that we we ran wolf like this and we know that it fails our test. And currently we're dealing with this thing where we've got maybe, you know, five or six or so tests in this file. And we don't want to run this test for the time being, but it's really long. We don't want to comment it out for whatever reason. You can really quickly just write dot skip here and you'll see that it does that little circle around it and shows that this is skipped and reports that it was skipped just so that someone doesn't accidentally leave that when someone tested in the future. That's a really nice little quick way to skip a test. What you can also do is let's say that you had multiple failing. OK, so we have two of these. I know I don't really need to do this, but, you know, you've got a bunch of tests that are failing and skipping it isn't enough. You're just trying to get to a passing state. And maybe you're trying to test the functionality of just one test. Well, let me show you. Let's say you've got that. You can do dot only here on the end of this one. And it'll only run that test. See, I just automatically added skip to every other one. And so that's that's really useful. Just so you know, this only does it per file. So it's it's going to do that for here. But if we had other tests for different files and stuff like that, those are going to be run. Well, let's show an example of that real quick. So I'm going to really quickly make a mu dot JS. And then what I'll do, actually, let me show you this. This is interesting. So I will make a mu dot test dot JS. And then in that, I'm going to make it. You shouldn't mix and match these normally. I'm just showing that you can. So I'll make it does the mu. And then that takes a function, et cetera, et cetera. Et cetera, et cetera. OK. So you'll see that it automatically pick that file up and run that test. Now, let's do this really quickly. I have a reason for what I'm doing here. Bear with me. I'm going to throw an error. OK, whatever. I don't care. That's going to throw that error. And I want to show that if we were trying to get around that error in our wolf tests here, let me do this. We're trying to throw an error to get around that that test. And I only want to do test only. Well, you can see that I left that there and it's not working. It's still.", "title": "JavaScript Testing Basics with Jest", "url": "https://www.youtube.com/watch?v=__QEPUdnJS0"}, {"text": "Still failing and still printing out only the failed one, because by default, just is going to put emphasis on what's failing. That kind of sucks. OK, so what we could do here. As we can use some watch usage, we're finally getting to this. We press W to show it and then you can press A to run all the tests again, etc, etc. But now we can do the filter. Let's see right here. Press P to filter by file name regex or you can press T to filter by file name regex. You can also press W to filter by a test name. So in this case, let's say we want to do a test. So I'm going to do a test and then we have this test that shows should return the number of wolves. And it's actually the only one that has wolves in it. So maybe let's do that. Press wolves, enter. And look at that. So now it doesn't show those failing tests. But if you look at this right here, you'll see that it's checking for this this regular expression. Whoops. And that's your pattern. Now you can do the same thing with a file. You could you could press C to clear the filters. You'll see it starts failing again. And then you can also come back here and then let's say, well, I don't really care because I have dot only. So what I want to do is I just want to press P to check for files and then the file itself. Well, that is the wolf file. And there you go. Now you get all of the tests in that file and it'll stop failing on your mute.js there. This is really, really handy. Now, the reason I made that extra file was because I want to show you and enlarge again. That's annoying. When I do coverage, coverage. That notice that the mu is not in here. OK, it's only checking Wolf dot JS currently. And the reason why it only has this lower coverage is because we are only testing that one test. We've skipped all the other tests that actually would make this coverage complete. Let me let me show that again. So let's get rid of that just real quick, just so you can see. And we'll run coverage again. And there you go. So our Wolf's great, but we're not even tracking mu. And that's kind of interesting because it does fail. Where is that here? It is failing this test here, interestingly. So why that is, is because we aren't ever importing it. So by default, the coverage report is only going to show. Well, we don't even have anything in this file, right? So let's let's just say const mu equals some string. You know, this is pointless, of course. Module dot exports. Mu. Show that again. You could see the only Wolf is there. And if we go over to here, we go back. We'll only see that one file. And then now we'll say const mu equals require and then dot slash mu. Now, as soon as we import that, let's see if it does anything. Look at that. It automatically popped up and said that it's 100 percent covered because it's just a simple string that was imported. And so that's what it does. Now, if we are really quickly to change it to a function instead. So let's say that it's a function instead that returns that. We'll run this report again. We'll see that it's not covered now. Zero functions because we never called it. OK, so that's that's the gist of how that works. When you are without any configuration, that's that's important because there is a just dot config that you can set up for this. It's going to only run the coverage report on the things that you import into your tests and actually have. But you can set it up to check for, let's say, the whole source directory. You can set specific ones, folders not to look into and to look into. And I'll probably go over that in the coverage video that I'm planning on doing. But it's a little bit much to get into right now. But just know that that's the case. So you could you do a really cool thing where, you know, you check that all of the JavaScript files in your source are covered in some way in your tests if you want to. And a common question that comes up there that I'll answer right here quickly and then we'll move on is how much coverage should you have? Because you might look at these numbers and be like, well, what does this even mean? You know, what should it be? You might not know. There's no hard and fast rule for that. You will typically see in my experience with other languages trying to aim for like an 80 percent threshold. That's just really generic and it varies by project. Just so you know, in just setups that I've seen, you typically want a little closer to 90 to 100 percent, depending on your project, because probably due to snapshots, another functionality that just gives you to cover things a little bit more easily. So just so that you know, you're trying to get it. It's a goal to get as high as you can. But there are diminishing returns and you typically don't go for 100 percent. I know some people strongly debate against that and it really depends on your project. But just so you know, it's just the more tests you have, you know, the more comfortable you can be that your code is working correctly. But don't overdo it to the point where you have to test every single little thing possible because there's sort of a diminishing return there. You can just do some reasonable default checks and be fine. Don't let that be a barrier to you getting into testing is what I'm trying to say. You can add those things as you go. And the best thing about starting with coverage now and adding tests as you go is that once you have like the basic building blocks, you can see as things are starting to get out of control and then you can use that as a mental reminder. So you could see that Mew or Woof ends up getting to like 30 statements covered after a certain amount of time. And that's a good flag for you to just be like, I should probably write some more tests because we're getting a little bit crazy here. It's kind of a hot topic. I'm going to leave it at that. All right, well, let's wrap this up by going over snapshots, which is a really cool functionality of Jest. To showcase snapshots, I'm going to reference some code that's a little bit more involved than I used in my git bisect video. And let me show you how this works real quick. When I run start, start command, all it does is it simply prints out this invoice. And you don't really need to know a lot about how the code works for this example. All it is is that it runs a series of different functions and then it ends up printing out the sort of invoice statement. This is actually taken from Martin Fowler's refactoring book. Now, I really should test all the individual functions there, but I'm going to show one of the cool things about snapshots is how you can really quickly get a test up and running that shows you the difference between outputs when you run tests. Let me show you that. So if we do expect and we do first invoice here, we can do to match snapshot. Now, when we run that, we'll run NPM T. And you can see it says one snapshot written. And if we take a look over here, there's this new snapshots directory. And if we look into it, it's pretty interesting is that we have the essentially the exact same output here printed out for this export. So how is that useful? Well, if we go over to the invoice printer, I want to show the mistake that I showed in that example. There was, you know, while we're refactoring our code, we moved into a new function. We accidentally left off that minus 30 and we didn't need. Now, I want to show this first is that, you know, if you're doing NPM start and you run this, especially if you're you're doing a lot of changes or it's been a long day or whatever, and you're kind of brain fatigued. It's very easy to miss the differences between these outputs between these outputs here. So you like as you're comparing them, 137 credits, oh, 47 credits. So this kind of 47 to 137 got messed up there, but the rest seems to be OK. Right. So come back to our test here. What if we just run NPM T again? Boom. Look at that. So it immediately. Went and checked our snapshot, it prints it out in a different formatted color now is pretty cool. And then it highlights the difference. So, you know, the rest of this was all the same. But here it automatically tested this for us based on the output. We didn't have to individually run each of those functions. Now, you got to be real careful here. I have to start with a disclaimer because this is a really hot topic in the testing community, especially JavaScript here with snapshots and all that. That snapshots are not where you should aim to be for all of your tests. And by that, I mean, don't use it to skip testing out individual functionality. It is nice to get a test up and running because what it essentially is telling you is that your test does run your code and does return a consistent output based on the previous one. But it doesn't necessarily know whether that output is intended or not. So in this case. We ran this test. You can see it said inspect your code changes to run NPM test dash dash you to update them. And you can also do this in watch mode. Let me show you. So when you're in this mode, there's a you command to update the failing tests. And so if you're not careful, you might come in here and you have like a lot of different dips or something like that. And you kind of get overwhelmed once you have a ton of snapshots going on. You'll just press you automatically and update it. Be like, oh, yeah, yeah, that was what I expected. And you won't read each individual line. It happens. Trust me. And that's bad because now you have a fail in your code that's not really testable. Let me show you. I just press you. And now it just thinks that that's the correct output.\n\nWe've just moved on and we think that we have passing tests. So I just want to show up throughout there that there's a little bit of a disclaimer that needs to be said that it can be a little bit dangerous to just, you know, go willy nilly on all snapshots. So this actually, to me, though, is a pretty cool example of how to start with just a real quick test snapshot, because as you could see from that one before, let's change it back. It immediately caught this regression is what that's called. You know, the code went back and performed performance or functionality or reliability in some way. So I want to update this to say that. Sorry. And then I'll fix this back. So let me show you this real quick. This is really neat for small code examples like that. This is perfect for two match inline snapshot. And I really love adding this to some tests. You can see that it made one snapshot obsolete. That's the one that was in this directory here before. So it wants us to press you to say, yes, that is obsolete. That's fine. And then that those snapshots go away now for that particular instance. But now it's printed out here live. Isn't that crazy? So it takes the result and it prints it out as a string and automatically updates this file. So if we were to go over here and we were to change this, I'll show you before I save. Notice it says you earned 47 credits down here. We're going to save that. Flip back on over here. And then notice it shows the difference here between the two. And then I press you to update and my code automatically updated in the actual test file itself to 137 credits. That's pretty crazy. So this is really nice to show like a quick documentation of what the output is when when it's very short or condensed and it's able to do that. You wouldn't want to do that for a gigantic CSS file or something like that. But for this instance, it's actually quite useful. So as you can see, snapshots are a really quick way to test whether the output of particular functions or anything like that is returning differently or whether you want to test that it's consistently outputting the same thing each time. And I already gave all the disclaimers that you should be careful with how you use those. But they're very nice addition to some tests. Some people really don't like them at all and don't think that they should be existent. I've actually found them to be really useful, in particular when I have a React component library and some CSS. It can be a flood when you change a component and you have a bunch of test snapshots that fail because you change one line of CSS. But if you're diligent about that, it's a nice thing that can catch things that are really difficult to test otherwise, especially when it's just like output CSS or JavaScript, which you really probably shouldn't be testing that too much anyways. But either way, let's say you were to change a button and another component uses that button and you forgot about that or that it added a different styling and it overrides that style. So it can catch some things like that. And I particularly found it to be useful, although other people have obviously very heated different opinions about that. And that's fine. I just want to show it to you so that you know that you can use it as long as you know, great power, great responsibility, all that good stuff. So that wraps up everything that I wanted to go over for this particular video, Jest Basics. You know, there's a lot more to it, of course, you can never get it all. But I think that that gives you a pretty good intro into what Jest offers, a little bit about unit testing and how you use it and some of the different matchers and expectation objects and formats and how you typically use it in an idiomatic way. But I do plan on making more Jest videos where I go a little bit more into configuration and some general setups and then use cases and all that type of stuff for more of the functionality of Jest if you're interested in that. So stay tuned for that. And of course, you can hit me up on Twitter if you have any individual requests about Jest or anything like that. But that'll conclude this video. I hope it was useful to you and thanks for watching.", "title": "JavaScript Testing Basics with Jest", "url": "https://www.youtube.com/watch?v=__QEPUdnJS0"}]